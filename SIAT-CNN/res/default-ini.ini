[Neural Network Parameters]

Initial learning rate (eta) = 0.001
Minimum learning rate (eta) = 0.00005
Rate of decay for learning rate (eta) = 0.794183335    ;;; 0.794183335 = 0.001 down to 0.00005 in 13 epochs
Decay rate is applied after this number of backprops = 2000
Number of backprop threads = 2
Number of testing threads = 1
Number of patterns used to calculate Hessian = 500
Limiting divisor (micron) for learning rate amplification (like 0.10 for 10x limit) = 0.10


[LFW Database Parameters]

Training images item count = 1000
Testing images item count = 100

Rows per image = 39
Columns per image = 39


[Parameters for Controlling Pattern Distortion During Backpropagation]

Maximum scale factor change (percent, like 20.0 for 20%) = 15.0
Maximum rotational change (degrees, like 20.0 for 20 degrees) = 15.0
Sigma for elastic distortions (higher numbers are more smooth and less distorted; Simard uses 4.0) = 8.0
Scaling for elastic distortions (higher numbers amplify distortions; Simard uses 0.34) = 0.5
   